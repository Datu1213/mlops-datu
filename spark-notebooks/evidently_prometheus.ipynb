{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6531a64d-b258-47ab-980c-a3b51bc4b643",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: hive.metastore.warehouse.dir\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/conda/lib/python3.11/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /opt/spark/.ivy2/cache\n",
      "The jars for the packages stored in: /opt/spark/.ivy2/jars\n",
      "org.postgresql#postgresql added as a dependency\n",
      "io.delta#delta-spark_2.12 added as a dependency\n",
      "io.delta#delta-storage added as a dependency\n",
      "org.elasticsearch#elasticsearch-spark-30_2.12 added as a dependency\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      "com.amazonaws#aws-java-sdk-bundle added as a dependency\n",
      "com.amazonaws#aws-java-sdk-bundle added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-864c920f-cf1e-4c80-97cc-2784ed658503;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.postgresql#postgresql;42.7.3 in central\n",
      "\tfound org.checkerframework#checker-qual;3.42.0 in central\n",
      "\tfound io.delta#delta-spark_2.12;3.3.0 in central\n",
      "\tfound io.delta#delta-storage;3.3.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.9.3 in central\n",
      "\tfound org.elasticsearch#elasticsearch-spark-30_2.12;8.11.3 in central\n",
      "\tfound org.scala-lang#scala-reflect;2.12.8 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.6 in central\n",
      "\tfound commons-logging#commons-logging;1.1.1 in central\n",
      "\tfound javax.xml.bind#jaxb-api;2.3.1 in central\n",
      "\tfound com.google.protobuf#protobuf-java;2.5.0 in central\n",
      "\tfound org.apache.spark#spark-yarn_2.12;3.3.3 in central\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.7 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.7 in central\n",
      "\tfound org.apache.kafka#kafka-clients;3.4.1 in central\n",
      "\tfound org.lz4#lz4-java;1.8.0 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.10.5 in central\n",
      "\tfound org.slf4j#slf4j-api;2.0.7 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.4 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.4 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.11.1 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.12.262 in central\n",
      ":: resolution report :: resolve 412ms :: artifacts dl 17ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.12.262 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java;2.5.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\tio.delta#delta-spark_2.12;3.3.0 from central in [default]\n",
      "\tio.delta#delta-storage;3.3.0 from central in [default]\n",
      "\tjavax.xml.bind#jaxb-api;2.3.1 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.9.3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.11.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;3.4.1 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.5.7 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.7 from central in [default]\n",
      "\torg.apache.spark#spark-yarn_2.12;3.3.3 from central in [default]\n",
      "\torg.checkerframework#checker-qual;3.42.0 from central in [default]\n",
      "\torg.elasticsearch#elasticsearch-spark-30_2.12;8.11.3 from central in [default]\n",
      "\torg.lz4#lz4-java;1.8.0 from central in [default]\n",
      "\torg.postgresql#postgresql;42.7.3 from central in [default]\n",
      "\torg.scala-lang#scala-reflect;2.12.8 from central in [default]\n",
      "\torg.slf4j#slf4j-api;2.0.7 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.10.5 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\torg.slf4j#slf4j-api;1.7.6 by [org.slf4j#slf4j-api;2.0.7] in [default]\n",
      "\tcommons-logging#commons-logging;1.1.1 by [commons-logging#commons-logging;1.1.3] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   24  |   0   |   0   |   2   ||   22  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-864c920f-cf1e-4c80-97cc-2784ed658503\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 22 already retrieved (0kB/8ms)\n",
      "25/11/11 09:30:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/11/11 09:31:05 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[user_id: string, event_type: string, page: string, purchase_value: double, ts: timestamp]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"mlflow\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .config(\"spark.executor.cores\", \"2\") \\\n",
    "    .config(\"spark.cores.max\", \"2\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "silver_df = spark.table(\"default.stg_user_events\")\n",
    "    \n",
    "# # 触发一次计算并将其缓存到Spark内存中\n",
    "silver_df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "feef60e3-66fb-4ec7-882f-b794ce77072a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.jars.packages',\n",
       "  'org.postgresql:postgresql:42.7.3,io.delta:delta-spark_2.12:3.3.0,io.delta:delta-storage:3.3.0,org.elasticsearch:elasticsearch-spark-30_2.12:8.11.3,org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.7,com.amazonaws:aws-java-sdk-bundle:1.12.262,com.amazonaws:aws-java-sdk-bundle:1.12.262'),\n",
       " ('spark.network.timeout', '600s'),\n",
       " ('spark.hadoop.javax.jdo.option.ConnectionDriverName',\n",
       "  'org.postgresql.Driver'),\n",
       " ('spark.hadoop.fs.s3a.path.style.access', 'true'),\n",
       " ('spark.files',\n",
       "  'file:///opt/spark/.ivy2/jars/org.postgresql_postgresql-42.7.3.jar,file:///opt/spark/.ivy2/jars/io.delta_delta-spark_2.12-3.3.0.jar,file:///opt/spark/.ivy2/jars/io.delta_delta-storage-3.3.0.jar,file:///opt/spark/.ivy2/jars/org.elasticsearch_elasticsearch-spark-30_2.12-8.11.3.jar,file:///opt/spark/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.7.jar,file:///opt/spark/.ivy2/jars/com.amazonaws_aws-java-sdk-bundle-1.12.262.jar,file:///opt/spark/.ivy2/jars/org.checkerframework_checker-qual-3.42.0.jar,file:///opt/spark/.ivy2/jars/org.antlr_antlr4-runtime-4.9.3.jar,file:///opt/spark/.ivy2/jars/org.scala-lang_scala-reflect-2.12.8.jar,file:///opt/spark/.ivy2/jars/javax.xml.bind_jaxb-api-2.3.1.jar,file:///opt/spark/.ivy2/jars/com.google.protobuf_protobuf-java-2.5.0.jar,file:///opt/spark/.ivy2/jars/org.apache.spark_spark-yarn_2.12-3.3.3.jar,file:///opt/spark/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.7.jar,file:///opt/spark/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar,file:///opt/spark/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar,file:///opt/spark/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar,file:///opt/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar,file:///opt/spark/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar,file:///opt/spark/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.5.jar,file:///opt/spark/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar,file:///opt/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar,file:///opt/spark/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar'),\n",
       " ('spark.hadoop.javax.jdo.option.ConnectionURL',\n",
       "  'jdbc:postgresql://postgres:5432/hive_metastore'),\n",
       " ('spark.app.submitTime', '1762566962659'),\n",
       " ('spark.core.connection.ack.wait.timeout', '600s'),\n",
       " ('spark.app.name', 'mlflow'),\n",
       " ('spark.master', 'spark://spark-master:7077'),\n",
       " ('spark.hadoop.fs.s3a.access.key', 'minioadmin'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.driver.host', '9ade425de2a0'),\n",
       " ('spark.sql.warehouse.dir', 'file:/opt/hive/warehouse'),\n",
       " ('spark.hadoop.fs.s3a.secret.key', 'minioadmin'),\n",
       " ('spark.driver.extraJavaOptions',\n",
       "  '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false'),\n",
       " ('spark.driver.port', '39959'),\n",
       " ('spark.submit.pyFiles',\n",
       "  '/opt/spark/.ivy2/jars/org.postgresql_postgresql-42.7.3.jar,/opt/spark/.ivy2/jars/io.delta_delta-spark_2.12-3.3.0.jar,/opt/spark/.ivy2/jars/io.delta_delta-storage-3.3.0.jar,/opt/spark/.ivy2/jars/org.elasticsearch_elasticsearch-spark-30_2.12-8.11.3.jar,/opt/spark/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.7.jar,/opt/spark/.ivy2/jars/com.amazonaws_aws-java-sdk-bundle-1.12.262.jar,/opt/spark/.ivy2/jars/org.checkerframework_checker-qual-3.42.0.jar,/opt/spark/.ivy2/jars/org.antlr_antlr4-runtime-4.9.3.jar,/opt/spark/.ivy2/jars/org.scala-lang_scala-reflect-2.12.8.jar,/opt/spark/.ivy2/jars/javax.xml.bind_jaxb-api-2.3.1.jar,/opt/spark/.ivy2/jars/com.google.protobuf_protobuf-java-2.5.0.jar,/opt/spark/.ivy2/jars/org.apache.spark_spark-yarn_2.12-3.3.3.jar,/opt/spark/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.7.jar,/opt/spark/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar,/opt/spark/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar,/opt/spark/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar,/opt/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar,/opt/spark/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar,/opt/spark/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.5.jar,/opt/spark/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar,/opt/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar,/opt/spark/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.app.startTime', '1762566962794'),\n",
       " ('spark.executor.cores', '2'),\n",
       " ('spark.jars.ivy', '/opt/spark/.ivy2'),\n",
       " ('spark.hadoop.fs.s3a.impl', 'org.apache.hadoop.fs.s3a.S3AFileSystem'),\n",
       " ('spark.repl.local.jars',\n",
       "  'file:///opt/spark/.ivy2/jars/org.postgresql_postgresql-42.7.3.jar,file:///opt/spark/.ivy2/jars/io.delta_delta-spark_2.12-3.3.0.jar,file:///opt/spark/.ivy2/jars/io.delta_delta-storage-3.3.0.jar,file:///opt/spark/.ivy2/jars/org.elasticsearch_elasticsearch-spark-30_2.12-8.11.3.jar,file:///opt/spark/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.7.jar,file:///opt/spark/.ivy2/jars/com.amazonaws_aws-java-sdk-bundle-1.12.262.jar,file:///opt/spark/.ivy2/jars/org.checkerframework_checker-qual-3.42.0.jar,file:///opt/spark/.ivy2/jars/org.antlr_antlr4-runtime-4.9.3.jar,file:///opt/spark/.ivy2/jars/org.scala-lang_scala-reflect-2.12.8.jar,file:///opt/spark/.ivy2/jars/javax.xml.bind_jaxb-api-2.3.1.jar,file:///opt/spark/.ivy2/jars/com.google.protobuf_protobuf-java-2.5.0.jar,file:///opt/spark/.ivy2/jars/org.apache.spark_spark-yarn_2.12-3.3.3.jar,file:///opt/spark/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.7.jar,file:///opt/spark/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar,file:///opt/spark/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar,file:///opt/spark/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar,file:///opt/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar,file:///opt/spark/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar,file:///opt/spark/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.5.jar,file:///opt/spark/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar,file:///opt/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar,file:///opt/spark/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar'),\n",
       " ('spark.jars',\n",
       "  'file:///opt/spark/.ivy2/jars/org.postgresql_postgresql-42.7.3.jar,file:///opt/spark/.ivy2/jars/io.delta_delta-spark_2.12-3.3.0.jar,file:///opt/spark/.ivy2/jars/io.delta_delta-storage-3.3.0.jar,file:///opt/spark/.ivy2/jars/org.elasticsearch_elasticsearch-spark-30_2.12-8.11.3.jar,file:///opt/spark/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.7.jar,file:///opt/spark/.ivy2/jars/com.amazonaws_aws-java-sdk-bundle-1.12.262.jar,file:///opt/spark/.ivy2/jars/org.checkerframework_checker-qual-3.42.0.jar,file:///opt/spark/.ivy2/jars/org.antlr_antlr4-runtime-4.9.3.jar,file:///opt/spark/.ivy2/jars/org.scala-lang_scala-reflect-2.12.8.jar,file:///opt/spark/.ivy2/jars/javax.xml.bind_jaxb-api-2.3.1.jar,file:///opt/spark/.ivy2/jars/com.google.protobuf_protobuf-java-2.5.0.jar,file:///opt/spark/.ivy2/jars/org.apache.spark_spark-yarn_2.12-3.3.3.jar,file:///opt/spark/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.7.jar,file:///opt/spark/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar,file:///opt/spark/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar,file:///opt/spark/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar,file:///opt/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar,file:///opt/spark/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar,file:///opt/spark/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.5.jar,file:///opt/spark/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar,file:///opt/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar,file:///opt/spark/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar'),\n",
       " ('spark.app.initial.jar.urls',\n",
       "  'spark://9ade425de2a0:39959/jars/io.delta_delta-storage-3.3.0.jar,spark://9ade425de2a0:39959/jars/com.google.code.findbugs_jsr305-3.0.0.jar,spark://9ade425de2a0:39959/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar,spark://9ade425de2a0:39959/jars/org.postgresql_postgresql-42.7.3.jar,spark://9ade425de2a0:39959/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.7.jar,spark://9ade425de2a0:39959/jars/org.apache.spark_spark-yarn_2.12-3.3.3.jar,spark://9ade425de2a0:39959/jars/org.apache.kafka_kafka-clients-3.4.1.jar,spark://9ade425de2a0:39959/jars/com.google.protobuf_protobuf-java-2.5.0.jar,spark://9ade425de2a0:39959/jars/org.xerial.snappy_snappy-java-1.1.10.5.jar,spark://9ade425de2a0:39959/jars/org.slf4j_slf4j-api-2.0.7.jar,spark://9ade425de2a0:39959/jars/commons-logging_commons-logging-1.1.3.jar,spark://9ade425de2a0:39959/jars/com.amazonaws_aws-java-sdk-bundle-1.12.262.jar,spark://9ade425de2a0:39959/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar,spark://9ade425de2a0:39959/jars/org.scala-lang_scala-reflect-2.12.8.jar,spark://9ade425de2a0:39959/jars/org.checkerframework_checker-qual-3.42.0.jar,spark://9ade425de2a0:39959/jars/io.delta_delta-spark_2.12-3.3.0.jar,spark://9ade425de2a0:39959/jars/org.lz4_lz4-java-1.8.0.jar,spark://9ade425de2a0:39959/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.7.jar,spark://9ade425de2a0:39959/jars/org.apache.commons_commons-pool2-2.11.1.jar,spark://9ade425de2a0:39959/jars/javax.xml.bind_jaxb-api-2.3.1.jar,spark://9ade425de2a0:39959/jars/org.elasticsearch_elasticsearch-spark-30_2.12-8.11.3.jar,spark://9ade425de2a0:39959/jars/org.antlr_antlr4-runtime-4.9.3.jar'),\n",
       " ('spark.app.initial.file.urls',\n",
       "  'spark://9ade425de2a0:39959/files/org.apache.commons_commons-pool2-2.11.1.jar,spark://9ade425de2a0:39959/files/org.elasticsearch_elasticsearch-spark-30_2.12-8.11.3.jar,spark://9ade425de2a0:39959/files/com.google.protobuf_protobuf-java-2.5.0.jar,spark://9ade425de2a0:39959/files/org.lz4_lz4-java-1.8.0.jar,spark://9ade425de2a0:39959/files/org.antlr_antlr4-runtime-4.9.3.jar,spark://9ade425de2a0:39959/files/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.7.jar,spark://9ade425de2a0:39959/files/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.7.jar,spark://9ade425de2a0:39959/files/commons-logging_commons-logging-1.1.3.jar,spark://9ade425de2a0:39959/files/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar,spark://9ade425de2a0:39959/files/javax.xml.bind_jaxb-api-2.3.1.jar,spark://9ade425de2a0:39959/files/org.xerial.snappy_snappy-java-1.1.10.5.jar,spark://9ade425de2a0:39959/files/org.checkerframework_checker-qual-3.42.0.jar,spark://9ade425de2a0:39959/files/io.delta_delta-spark_2.12-3.3.0.jar,spark://9ade425de2a0:39959/files/com.amazonaws_aws-java-sdk-bundle-1.12.262.jar,spark://9ade425de2a0:39959/files/org.apache.spark_spark-yarn_2.12-3.3.3.jar,spark://9ade425de2a0:39959/files/com.google.code.findbugs_jsr305-3.0.0.jar,spark://9ade425de2a0:39959/files/org.scala-lang_scala-reflect-2.12.8.jar,spark://9ade425de2a0:39959/files/org.apache.hadoop_hadoop-client-api-3.3.4.jar,spark://9ade425de2a0:39959/files/org.postgresql_postgresql-42.7.3.jar,spark://9ade425de2a0:39959/files/io.delta_delta-storage-3.3.0.jar,spark://9ade425de2a0:39959/files/org.apache.kafka_kafka-clients-3.4.1.jar,spark://9ade425de2a0:39959/files/org.slf4j_slf4j-api-2.0.7.jar'),\n",
       " ('spark.sql.catalogImplementation', 'hive'),\n",
       " ('spark.sql.extensions', 'io.delta.sql.DeltaSparkSessionExtension'),\n",
       " ('spark.rpc.message.maxSize', '512'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.executor.extraJavaOptions',\n",
       "  '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false'),\n",
       " ('spark.cores.max', '2'),\n",
       " ('spark.hadoop.javax.jdo.option.ConnectionUserName', 'hive'),\n",
       " ('spark.hadoop.fs.s3a.endpoint', 'http://minio:9000'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.hadoop.javax.jdo.option.ConnectionPassword', 'hive'),\n",
       " ('spark.sql.catalog.spark_catalog',\n",
       "  'org.apache.spark.sql.delta.catalog.DeltaCatalog'),\n",
       " ('spark.app.id', 'app-20251108015603-0000')]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 获取所有配置项\n",
    "spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7049947e-c342-42b1-a65c-cc4751f4bbe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "成功找到 'silver_df' (包含 45654 行数据)。\n",
      "正在将 Spark DataFrame 转换为 Pandas DataFrame...\n",
      "转换完成。\n",
      "总行数: 45654\n",
      "参考数据集 (前50%): 22827 行\n",
      "当前数据集 (后50%): 22827 行\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from evidently import Report\n",
    "from evidently.presets import DataDriftPreset\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# --- 2. 确认 'silver_df' 存在 ---\n",
    "if 'silver_df' not in locals():\n",
    "    print(\"错误：未找到 Spark DataFrame 'silver_df'。\")\n",
    "    print(\"请先运行之前的单元格来加载数据 (spark.table('default.stg_user_events'))。\")\n",
    "else:\n",
    "    print(f\"成功找到 'silver_df' (包含 {silver_df.count()} 行数据)。\")\n",
    "\n",
    "    # --- 3. 将 Spark DataFrame 转换为 Pandas DataFrame ---\n",
    "    # 警告：对于超大数据集 (TB级)，我们应该使用 .sample() 进行采样。\n",
    "    # 对于我们的45k行数据，.toPandas() 是可以接受的。\n",
    "    print(\"正在将 Spark DataFrame 转换为 Pandas DataFrame...\")\n",
    "    all_data_pd = silver_df.toPandas()\n",
    "    print(\"转换完成。\")\n",
    "\n",
    "    # --- 4. 模拟“参考”数据集和“当前”数据集 ---\n",
    "    split_point = int(len(all_data_pd) * 0.5)\n",
    "    reference_data_pd = all_data_pd.iloc[:split_point]\n",
    "    current_data_pd = all_data_pd.iloc[split_point:]\n",
    "\n",
    "    print(f\"总行数: {len(all_data_pd)}\")\n",
    "    print(f\"参考数据集 (前50%): {len(reference_data_pd)} 行\")\n",
    "    print(f\"当前数据集 (后50%): {len(current_data_pd)} 行\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e9deb903-6adc-4782-8536-04509efa836d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "正在生成数据漂移报告 (DataDriftPreset)...\n",
      "\n",
      "--- 成功！---\n",
      "数据漂移报告已保存到: /home/jovyan/work/data_drift_report.html\n",
      "\n",
      "请在JupyterLab的左侧文件浏览器中，找到 'work/' 目录下的 'data_drift_report.html' 文件，\n",
      "右键点击它并选择 'Open in New Browser Tab' (在新浏览器标签页中打开) 来查看。\n"
     ]
    }
   ],
   "source": [
    "# --- 5. 创建 Evidently 漂移报告 ---\n",
    "print(\"\\n正在生成数据漂移报告 (DataDriftPreset)...\")\n",
    "\n",
    "# DataDriftPreset 会自动分析所有列，包括我们的\"ts\"（时间戳）\n",
    "# \"purchase_value\"（数值）和 \"event_type\"（分类）\n",
    "data_drift_report = Report(metrics=[\n",
    "    DataDriftPreset(),\n",
    "])\n",
    "\n",
    "# 运行计算！\n",
    "my_report = data_drift_report.run(\n",
    "    current_data=current_data_pd, \n",
    "    reference_data=reference_data_pd, \n",
    ")\n",
    "\n",
    "# --- 6. 将报告保存为 HTML 文件 ---\n",
    "report_path = \"/home/jovyan/work/data_drift_report.html\"\n",
    "my_report.save_html(report_path)\n",
    "\n",
    "print(\"\\n--- 成功！---\")\n",
    "print(f\"数据漂移报告已保存到: {report_path}\")\n",
    "print(\"\\n请在JupyterLab的左侧文件浏览器中，找到 'work/' 目录下的 'data_drift_report.html' 文件，\")\n",
    "print(\"右键点击它并选择 'Open in New Browser Tab' (在新浏览器标签页中打开) 来查看。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "270bc2fb-99d0-4215-9317-8ad9b1511f20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'metrics': [{'id': '15e89f895b482f9b84ba7274ed18a106', 'metric_id': 'DriftedColumnsCount(drift_share=0.5)', 'value': {'count': 0.0, 'share': 0.0}}, {'id': '6e335cb10cdff7116c9a2779505312b9', 'metric_id': 'ValueDrift(column=purchase_value)', 'value': np.float64(0.01365116851194981)}, {'id': '330639c072ebcb0f60d3d208bb6f9b22', 'metric_id': 'ValueDrift(column=user_id)', 'value': np.float64(0.007919947910332509)}, {'id': '2f3ddb08b4459fb21120259cb2aa8302', 'metric_id': 'ValueDrift(column=event_type)', 'value': np.float64(0.0048321714239055)}, {'id': '5ff6a229a83c51b4cd78218a3971d31c', 'metric_id': 'ValueDrift(column=page)', 'value': np.float64(0.0066110243858271585)}], 'tests': []}\n"
     ]
    }
   ],
   "source": [
    "print(my_report.dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2321f154-d01e-4b97-a126-e6d4d1dfadd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "参考数据集 (reference_data_pd) 已准备好。\n",
      "正在创建“被污染”的当前数据集 (current_data_pd_corrupted)...\n",
      "...已模拟 'purchase_value' 的数值漂移。\n",
      "...已模拟 'event_type' 的分类漂移（引入了 'search'）。\n",
      "\n",
      "正在生成“被污染”的数据漂移报告...\n",
      "\n",
      "--- 成功！---\n",
      "“被污染”的数据漂移报告已保存到: /home/jovyan/work/data_drift_report_CORRUPTED.html\n",
      "\n",
      "请在JupyterLab中打开这个新报告 ('data_drift_report_CORRUPTED.html')。\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Example report.dict() struct\n",
    "# {'metrics': [{'id': '15e89f895b482f9b84ba7274ed18a106', 'metric_id': 'DriftedColumnsCount(drift_share=0.5)', 'value': {'count': 0.0, 'share': 0.0}}, {'id': '6e335cb10cdff7116c9a2779505312b9', 'metric_id': 'ValueDrift(column=purchase_value)', 'value': np.float64(0.01365116851194981)}, {'id': '330639c072ebcb0f60d3d208bb6f9b22', 'metric_id': 'ValueDrift(column=user_id)', 'value': np.float64(0.007919947910332509)}, {'id': '2f3ddb08b4459fb21120259cb2aa8302', 'metric_id': 'ValueDrift(column=event_type)', 'value': np.float64(0.0048321714239055)}, {'id': '5ff6a229a83c51b4cd78218a3971d31c', 'metric_id': 'ValueDrift(column=page)', 'value': np.float64(0.0066110243858271585)}], 'tests': []}\n",
    "\n",
    "\n",
    "\n",
    "# --- 1. 确保我们的“黄金标准”参考数据已准备好 ---\n",
    "if 'reference_data_pd' not in locals():\n",
    "    print(\"错误：未找到 'reference_data_pd'。请先运行上一个单元格。\")\n",
    "else:\n",
    "    print(\"参考数据集 (reference_data_pd) 已准备好。\")\n",
    "\n",
    "    # --- 2. 创建一个“被污染”的当前数据集 ---\n",
    "    print(\"正在创建“被污染”的当前数据集 (current_data_pd_corrupted)...\")\n",
    "    current_data_pd_corrupted = all_data_pd.iloc[split_point:].copy()\n",
    "\n",
    "    # a. 模拟“数值漂移” (Numeric Drift)\n",
    "    # 假设由于通货膨胀或促销，所有 'purchase_value' 都显著增加了\n",
    "    # (我们只对非NULL值进行操作，以避免类型错误)\n",
    "    mask = current_data_pd_corrupted['purchase_value'].notnull()\n",
    "    current_data_pd_corrupted.loc[mask, 'purchase_value'] = current_data_pd_corrupted.loc[mask, 'purchase_value'] * 3 + 50\n",
    "    print(\"...已模拟 'purchase_value' 的数值漂移。\")\n",
    "\n",
    "    # b. 模拟“分类漂移” (Categorical Drift)\n",
    "    # 假设网站改版，出现了一个模型从未见过的新事件类型 'search'\n",
    "    # 我们将 30% 的 'click' 事件替换为 'search'\n",
    "    click_indices = current_data_pd_corrupted[current_data_pd_corrupted['event_type'] == 'click'].index\n",
    "    indices_to_replace = np.random.choice(click_indices, size=int(len(click_indices) * 0.3), replace=False)\n",
    "    current_data_pd_corrupted.loc[indices_to_replace, 'event_type'] = 'search'\n",
    "    print(\"...已模拟 'event_type' 的分类漂移（引入了 'search'）。\")\n",
    "\n",
    "\n",
    "    # --- 3. 运行新的漂移报告 ---\n",
    "    print(\"\\n正在生成“被污染”的数据漂移报告...\")\n",
    "    \n",
    "    corrupted_data_drift_report = Report(metrics=[\n",
    "        DataDriftPreset(),\n",
    "    ])\n",
    "\n",
    "    corrupted_data_drift_report = corrupted_data_drift_report.run(\n",
    "        current_data=current_data_pd_corrupted, # <-- 使用被污染的数据\n",
    "        reference_data=reference_data_pd,      # <-- 使用原始的参考数据\n",
    "    )\n",
    "\n",
    "    # --- 4. 保存新的报告 ---\n",
    "    corrupted_report_path = \"/home/jovyan/work/data_drift_report_CORRUPTED.html\"\n",
    "    corrupted_data_drift_report.save_html(corrupted_report_path)\n",
    "\n",
    "    print(\"\\n--- 成功！---\")\n",
    "    print(f\"“被污染”的数据漂移报告已保存到: {corrupted_report_path}\")\n",
    "    print(\"\\n请在JupyterLab中打开这个新报告 ('data_drift_report_CORRUPTED.html')。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "365cb828-72cd-4f82-8138-4338e32de99e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'metrics': [{'id': '15e89f895b482f9b84ba7274ed18a106', 'metric_id': 'DriftedColumnsCount(drift_share=0.5)', 'value': {'count': 2.0, 'share': 0.5}}, {'id': '6e335cb10cdff7116c9a2779505312b9', 'metric_id': 'ValueDrift(column=purchase_value)', 'value': np.float64(5.554204100091303)}, {'id': '330639c072ebcb0f60d3d208bb6f9b22', 'metric_id': 'ValueDrift(column=user_id)', 'value': np.float64(0.007919947910332509)}, {'id': '2f3ddb08b4459fb21120259cb2aa8302', 'metric_id': 'ValueDrift(column=event_type)', 'value': np.float64(0.17060528922301263)}, {'id': '5ff6a229a83c51b4cd78218a3971d31c', 'metric_id': 'ValueDrift(column=page)', 'value': np.float64(0.0066110243858271585)}], 'tests': []}\n"
     ]
    }
   ],
   "source": [
    "print(corrupted_data_drift_report.dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "875241ef-23d2-485c-a63b-e37eb03a7792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "成功找到 'my_report' 变量，正在提取漂移分数...\n",
      "Find it\n",
      "\n",
      "--- 成功！---\n",
      "已将漂移指标推送到Prometheus Pushgateway:\n",
      "model_data_drift_score = 0.0\n",
      "model_data_drift_detected = 0.0\n"
     ]
    }
   ],
   "source": [
    "from prometheus_client import CollectorRegistry, Gauge, push_to_gateway\n",
    "import json\n",
    "\n",
    "# my_report:normal corrupted_data_drift_report:error\n",
    "reportname='my_report'\n",
    "# --- 1. 确保 'reportname' 变量存在于内存中 ---\n",
    "if reportname not in locals():\n",
    "    print(f\"错误：未在内存中找到 '{reportname}' 变量。\")\n",
    "    print(\"请重新运行上一个Evidently报告单元格。\")\n",
    "else:\n",
    "    print(f\"成功找到 '{reportname}' 变量，正在提取漂移分数...\")\n",
    "    \n",
    "    # --- 2. 从Evidently的JSON结果中提取漂移分数 ---\n",
    "    #    (使用你创建的 'my_report' 变量!)\n",
    "    drift_score = 0.0\n",
    "    drift_detected = 0 # 0 = false, 1 = true\n",
    "    try:\n",
    "        # 遍历所有指标 my_report:normal corrupted_data_drift_report:error\n",
    "        for metric in locals().get(reportname).dict().get(\"metrics\", []):\n",
    "            if 'DriftedColumnsCount' in metric.get(\"metric_id\"):\n",
    "                print(\"Find it\")\n",
    "                drift_score = metric.get(\"value\", {}).get(\"share\", 0.0)\n",
    "                drift_detected = metric.get(\"value\", {}).get(\"count\", 0)\n",
    "                break\n",
    "    except Exception as e:\n",
    "        print(f\"无法从Evidently报告中提取漂移分数: {e}\")\n",
    "\n",
    "    # --- 3. 准备要推送的指标 ---\n",
    "    registry = CollectorRegistry()\n",
    "    g_drift_score = Gauge(\n",
    "        'model_data_drift_score', \n",
    "        'Evidently AI data drift score', \n",
    "        ['model_name'], \n",
    "        registry=registry\n",
    "    )\n",
    "    g_drift_detected = Gauge(\n",
    "        'model_data_drift_detected', \n",
    "        'Whether Evidently AI detected drift (1 = True, 0 = False)', \n",
    "        ['model_name'], \n",
    "        registry=registry\n",
    "    )\n",
    "\n",
    "    # --- 4. 设置指标的值 ---\n",
    "    model_name = \"stg_user_events_v1\"\n",
    "    g_drift_score.labels(model_name=model_name).set(drift_score)\n",
    "    g_drift_detected.labels(model_name=model_name).set(drift_detected)\n",
    "\n",
    "    # --- 5. 推送到Pushgateway ---\n",
    "    try:\n",
    "        # 使用Docker网络中的服务名 'pushgateway' 和端口 '9091'\n",
    "        push_to_gateway('pushgateway:9091', job='evidently_batch_validation', registry=registry)\n",
    "        print(f\"\\n--- 成功！---\")\n",
    "        print(f\"已将漂移指标推送到Prometheus Pushgateway:\")\n",
    "        print(f\"model_data_drift_score = {drift_score}\")\n",
    "        print(f\"model_data_drift_detected = {drift_detected}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n--- 失败！---\")\n",
    "        print(f\"无法推送到Prometheus Pushgateway (http://pushgateway:9091): {e}\")\n",
    "        print(\"请确保Pushgateway容器正在运行，并且网络名称正确。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e477b506-96df-4605-8a21-a05cd80b8459",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
