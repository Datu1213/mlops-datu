{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "567446da-43fb-4e08-8581-08835cafcc31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLFLOW_TRACKING_URI set to: http://mlflow-server:5000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 1. é…ç½®MLflowå®¢æˆ·ç«¯\n",
    "# -----------------------------------\n",
    "# å‘Šè¯‰MLflowå®¢æˆ·ç«¯ï¼ŒTracking Serveråœ¨å“ªé‡Œ\n",
    "# (ä½¿ç”¨Dockerç½‘ç»œå†…éƒ¨çš„æœåŠ¡å)\n",
    "os.environ[\"MLFLOW_TRACKING_URI\"] = \"http://mlflow-server:5000\"\n",
    "\n",
    "# å‘Šè¯‰MLflowå®¢æˆ·ç«¯ï¼ŒS3(MinIO)äº§ç‰©å­˜å‚¨åœ¨å“ªé‡Œ\n",
    "# (è¿™æ˜¯è®©å®¢æˆ·ç«¯èƒ½ç›´æ¥ä¸Šä¼ åˆ°MinIOçš„å…³é”®)\n",
    "os.environ[\"MLFLOW_S3_ENDPOINT_URL\"] = \"http://minio:9000\"\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = \"minioadmin\"\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"minioadmin\"\n",
    "\n",
    "print(f\"MLFLOW_TRACKING_URI set to: {os.environ['MLFLOW_TRACKING_URI']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439aeabe-46a5-47fa-a6c3-e71356afc60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 2. è®¾ç½®å®éªŒ\n",
    "# -----------------------------------\n",
    "# å®éªŒå°†æŒ‰åç§°è‡ªåŠ¨åˆ›å»º\n",
    "experiment_name = \"my_first_ml_experiment\"\n",
    "mlflow.set_tracking_uri(\"http://mlflow-server:5000\")\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "print(f\"Using MLflow experiment: {experiment_name}\")\n",
    "\n",
    "# 3. å‡†å¤‡æ•°æ®å’Œå‚æ•°\n",
    "# -----------------------------------\n",
    "X, y = load_iris(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# æˆ‘ä»¬è¦è¯•éªŒçš„è¶…å‚æ•°\n",
    "model_params = {\n",
    "    \"C\": 0.003927893213212713,\n",
    "    \"solver\": \"saga\"\n",
    "}\n",
    "\n",
    "print(\"Data and parameters are ready.\")\n",
    "\n",
    "# 4. å¼€å§‹ä¸€ä¸ªMLflow \"Run\" (ä¸€æ¬¡å®éªŒ)\n",
    "# -----------------------------------\n",
    "with mlflow.start_run() as run:\n",
    "    print(f\"Starting run: {run.info.run_id}\")\n",
    "\n",
    "    # 5. è®­ç»ƒæ¨¡å‹\n",
    "    # -----------------------------------\n",
    "    model = LogisticRegression(C=model_params[\"C\"], solver=model_params[\"solver\"])\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    # 6. è®°å½•åˆ°MLflowæœåŠ¡å™¨\n",
    "    # -----------------------------------\n",
    "    print(\"Logging to MLflow...\")\n",
    "\n",
    "    # a. è®°å½•å‚æ•°\n",
    "    mlflow.log_params(model_params)\n",
    "    print(f\"Logged params: {model_params}\")\n",
    "\n",
    "    # b. è®°å½•æŒ‡æ ‡\n",
    "    mlflow.log_metric(\"accuracy\", accuracy)\n",
    "    print(f\"Logged metric 'accuracy': {accuracy}\")\n",
    "\n",
    "    # c. è®°å½•æ¨¡å‹æ–‡ä»¶\n",
    "    # (è¿™ä¼šè‡ªåŠ¨å°†æ¨¡å‹ä¿å­˜å¹¶ä¸Šä¼ åˆ°MinIO)\n",
    "    # \"model\" æ˜¯äº§ç‰©åœ¨MinIOä¸­çš„å­ç›®å½•å\n",
    "    mlflow.sklearn.log_model(model, name=\"model\")\n",
    "    print(\"Logged model to MinIO.\")\n",
    "\n",
    "print(\"\\n--- Experiment Run Finished! ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0812bc3-a9e3-4952-b417-f387679d3099",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# --- æˆ‘ä»¬å‡è£…ä¸Šä¸€å•å…ƒæ ¼åˆšè¿è¡Œå®Œ ---\n",
    "# run_id åº”è¯¥æ˜¯ä¸Šä¸€ä¸ªå•å…ƒæ ¼ä¸­ mlflow.start_run() äº§ç”Ÿçš„\n",
    "# (å¦‚æœå•å…ƒæ ¼å·²åœæ­¢ï¼Œä½ éœ€è¦å»UIä¸Šæ‰‹åŠ¨å¤åˆ¶é‚£ä¸ª run_id)\n",
    "# å‡è®¾ run_id å­˜å‚¨åœ¨å˜é‡ run.info.run_id ä¸­\n",
    "run_id = run.info.run_id \n",
    "model_uri = f\"runs:/{run_id}/model\"\n",
    "\n",
    "# --- è¿™æ˜¯æ–°å†…å®¹ï¼šæ³¨å†Œæ¨¡å‹ ---\n",
    "model_name = \"iris_logistic_regression\"\n",
    "\n",
    "print(f\"Registering model '{model_name}' from run '{run_id}'...\")\n",
    "\n",
    "# 1. æ³¨å†Œæ–°ç‰ˆæœ¬\n",
    "# è¿™ä¼šä»S3(MinIO)ä¸­è·å–æ¨¡å‹ï¼Œå¹¶å°†å…¶æ³¨å†Œåˆ°PostgreSQLå…ƒæ•°æ®ä¸­\n",
    "model_version = mlflow.register_model(\n",
    "    model_uri=model_uri,\n",
    "    name=model_name\n",
    ")\n",
    "\n",
    "print(f\"Model successfully registered as '{model_name}' version {model_version.version}\")\n",
    "time.sleep(5) # ç­‰å¾…æ³¨å†Œè¡¨ç”Ÿæ•ˆ\n",
    "\n",
    "# 2. Set Alias (Transition Stage)\n",
    "# FutureWarning: \n",
    "# ``mlflow.tracking.client.MlflowClient.transition_model_version_stage``\n",
    "# is deprecated since 2.9.0. \n",
    "# Model registry stages will be removed in a future major release. \n",
    "# Use Aliases instead.\n",
    "\n",
    "# Run it multiple times with different aliases as needed\n",
    "print(\"Transitioning model to 'Staging'...\")\n",
    "client = mlflow.MlflowClient()\n",
    "client.set_registered_model_alias(\n",
    "    name=model_name,\n",
    "    version=model_version.version,\n",
    "    alias=\"Staging\"\n",
    ")\n",
    "print(\"Model version successfully transitioned to 'Staging'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c7a4b12-7402-4980-a78d-8118ffbfbc31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/conda/lib/python3.11/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /opt/spark/.ivy2/cache\n",
      "The jars for the packages stored in: /opt/spark/.ivy2/jars\n",
      "org.postgresql#postgresql added as a dependency\n",
      "io.delta#delta-spark_2.12 added as a dependency\n",
      "io.delta#delta-storage added as a dependency\n",
      "org.elasticsearch#elasticsearch-spark-30_2.12 added as a dependency\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      "com.amazonaws#aws-java-sdk-bundle added as a dependency\n",
      "com.amazonaws#aws-java-sdk-bundle added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-c30dc937-9a47-4390-b322-48d55c2e9897;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.postgresql#postgresql;42.7.3 in central\n",
      "\tfound org.checkerframework#checker-qual;3.42.0 in central\n",
      "\tfound io.delta#delta-spark_2.12;3.3.0 in central\n",
      "\tfound io.delta#delta-storage;3.3.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.9.3 in central\n",
      "\tfound org.elasticsearch#elasticsearch-spark-30_2.12;8.11.3 in central\n",
      "\tfound org.scala-lang#scala-reflect;2.12.8 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.6 in central\n",
      "\tfound commons-logging#commons-logging;1.1.1 in central\n",
      "\tfound javax.xml.bind#jaxb-api;2.3.1 in central\n",
      "\tfound com.google.protobuf#protobuf-java;2.5.0 in central\n",
      "\tfound org.apache.spark#spark-yarn_2.12;3.3.3 in central\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.7 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.7 in central\n",
      "\tfound org.apache.kafka#kafka-clients;3.4.1 in central\n",
      "\tfound org.lz4#lz4-java;1.8.0 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.10.5 in central\n",
      "\tfound org.slf4j#slf4j-api;2.0.7 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.4 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.4 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.11.1 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.12.262 in central\n",
      ":: resolution report :: resolve 345ms :: artifacts dl 13ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.12.262 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java;2.5.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\tio.delta#delta-spark_2.12;3.3.0 from central in [default]\n",
      "\tio.delta#delta-storage;3.3.0 from central in [default]\n",
      "\tjavax.xml.bind#jaxb-api;2.3.1 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.9.3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.11.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;3.4.1 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.5.7 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.7 from central in [default]\n",
      "\torg.apache.spark#spark-yarn_2.12;3.3.3 from central in [default]\n",
      "\torg.checkerframework#checker-qual;3.42.0 from central in [default]\n",
      "\torg.elasticsearch#elasticsearch-spark-30_2.12;8.11.3 from central in [default]\n",
      "\torg.lz4#lz4-java;1.8.0 from central in [default]\n",
      "\torg.postgresql#postgresql;42.7.3 from central in [default]\n",
      "\torg.scala-lang#scala-reflect;2.12.8 from central in [default]\n",
      "\torg.slf4j#slf4j-api;2.0.7 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.10.5 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\torg.slf4j#slf4j-api;1.7.6 by [org.slf4j#slf4j-api;2.0.7] in [default]\n",
      "\tcommons-logging#commons-logging;1.1.1 by [commons-logging#commons-logging;1.1.3] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   24  |   0   |   0   |   2   ||   22  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-c30dc937-9a47-4390-b322-48d55c2e9897\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 22 already retrieved (0kB/8ms)\n",
      "25/11/13 00:43:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/11/13 00:43:59 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/11/13 00:43:59 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "25/11/13 00:43:59 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "25/11/13 00:43:59 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "hadoop_aws_pkg = \"org.apache.hadoop:hadoop-aws:3.3.4\" \n",
    "aws_java_sdk_pkg = \"com.amazonaws:aws-java-sdk-bundle:1.12.262\"\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"mlflow\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", \"/opt/hive/warehouse\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .config(\"spark.executor.cores\", \"2\") \\\n",
    "    .config(\"spark.cores.max\", \"2\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"minioadmin\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"minioadmin\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "    \n",
    "# print(f\"Successfully loaded and cached. Number of rows: {silver_df.count()}\")\n",
    "# silver_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01b27d55-2ba0-42e6-97d6-e5584bfd214c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/13 00:49:12 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[user_id: string, event_type: string, page: string, purchase_value: double, ts: timestamp]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "silver_df = spark.table(\"default.stg_user_events\")\n",
    "    \n",
    "# # è§¦å‘ä¸€æ¬¡è®¡ç®—å¹¶å°†å…¶ç¼“å­˜åˆ°Sparkå†…å­˜ä¸­\n",
    "silver_df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7acd2ecc-3c14-4834-837a-b2d02748d591",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'org.apache.hadoop.fs.s3a.S3AFileSystem'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get(\"spark.hadoop.fs.s3a.impl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "73a0c308-1b5f-421d-afd4-8485c51b38b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data prepared with 'label' column:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "| event_type|label|\n",
      "+-----------+-----+\n",
      "|   purchase|  1.0|\n",
      "|add_to_cart|  2.0|\n",
      "|  page_view|  0.0|\n",
      "|      click|  3.0|\n",
      "+-----------+-----+\n",
      "\n",
      "Spark ML Pipeline defined.\n",
      "Starting MLflow run: 92db72113001458dada9f1df43a6bf6a\n",
      "Training distributed model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/13 00:49:28 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.VectorBLAS\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging Spark ML Pipeline to MLflow...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/11/13 00:49:42 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /tmp/tmpiks6zv2p/model, flavor: spark). Fall back to return ['pyspark==3.5.7']. Set logging level to DEBUG to see the full traceback. \n",
      "\u001b[31m2025/11/13 00:49:42 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artifact URI: file:///home/jovyan/work/mlruns/844225054701675088/92db72113001458dada9f1df43a6bf6a/artifacts\n",
      "\n",
      "--- Spark ML Experiment Run Finished! ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/13 00:56:08 ERROR StandaloneSchedulerBackend: Application has been killed. Reason: Master removed our application: KILLED\n",
      "25/11/13 00:56:08 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exiting due to error from cluster scheduler: Master removed our application: KILLED\n",
      "\tat org.apache.spark.errors.SparkCoreErrors$.clusterSchedulerError(SparkCoreErrors.scala:291)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.error(TaskSchedulerImpl.scala:981)\n",
      "\tat org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend.dead(StandaloneSchedulerBackend.scala:165)\n",
      "\tat org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint.markDead(StandaloneAppClient.scala:263)\n",
      "\tat org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint$$anonfun$receive$1.applyOrElse(StandaloneAppClient.scala:170)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# 1. å‡†å¤‡æ•°æ® (å‡è®¾'silver_df'å˜é‡å·²å­˜åœ¨)\n",
    "# -----------------------------------\n",
    "# a. å¡«å…… purchase_value ä¸­çš„ NULL (MLlibä¸å–œæ¬¢NULL)\n",
    "# æˆ‘ä»¬ç”¨0.0æ¥å¡«å……\n",
    "features_df = silver_df.fillna(0.0, subset=['purchase_value'])\n",
    "\n",
    "# b. åˆ›å»ºæˆ‘ä»¬çš„\"æ ‡ç­¾\"(label)\n",
    "# æˆ‘ä»¬çš„ç›®æ ‡æ˜¯ï¼šé¢„æµ‹ event_type\n",
    "# æˆ‘ä»¬éœ€è¦å…ˆæŠŠå®ƒè½¬æ¢æˆä¸€ä¸ªæ•°å­—ç´¢å¼•\n",
    "label_indexer = StringIndexer(inputCol=\"event_type\", outputCol=\"label\")\n",
    "features_df = label_indexer.fit(features_df).transform(features_df)\n",
    "\n",
    "print(\"Data prepared with 'label' column:\")\n",
    "features_df.select(\"event_type\", \"label\").distinct().show()\n",
    "\n",
    "# 2. å®šä¹‰ç‰¹å¾è½¬æ¢å™¨ (StringIndexers)\n",
    "# -----------------------------------\n",
    "# æˆ‘ä»¬éœ€è¦å°†æ‰€æœ‰å­—ç¬¦ä¸²ç‰¹å¾åˆ—è½¬æ¢ä¸ºæ•°å­—\n",
    "categorical_cols = [\"user_id\", \"page\"]\n",
    "indexers = [\n",
    "    StringIndexer(inputCol=col, outputCol=f\"{col}_index\", handleInvalid=\"keep\") \n",
    "    for col in categorical_cols\n",
    "]\n",
    "\n",
    "# 3. å®šä¹‰ç‰¹å¾æ±‡ç¼–å™¨ (VectorAssembler)\n",
    "# -----------------------------------\n",
    "# å°†æ‰€æœ‰æ•°å­—ç‰¹å¾ç»„åˆæˆä¸€ä¸ªå‘é‡\n",
    "feature_cols = [f\"{col}_index\" for col in categorical_cols] + [\"purchase_value\"]\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "\n",
    "# 4. å®šä¹‰æ¨¡å‹\n",
    "# -----------------------------------\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\n",
    "\n",
    "# 5. æ„å»ºSpark MLç®¡é“\n",
    "# -----------------------------------\n",
    "# ç®¡é“ä¼šæŒ‰é¡ºåºè‡ªåŠ¨æ‰§è¡Œæ‰€æœ‰æ­¥éª¤\n",
    "pipeline = Pipeline(stages=indexers + [assembler, lr])\n",
    "\n",
    "print(\"Spark ML Pipeline defined.\")\n",
    "\n",
    "# 6. ä½¿ç”¨MLflowè·Ÿè¸ªå¹¶è®­ç»ƒæ¨¡å‹\n",
    "# -----------------------------------\n",
    "# (ç¡®ä¿ä½ ä¹‹å‰çš„MLflowé…ç½®å•å…ƒæ ¼å·²è¿è¡Œ)\n",
    "import mlflow.spark # å¯¼å…¥Sparkä¸“ç”¨çš„MLflowæ¨¡å—\n",
    "\n",
    "mlflow.set_experiment(\"spark_ml_experiment\") # åˆ›å»ºä¸€ä¸ªæ–°å®éªŒ\n",
    "\n",
    "with mlflow.start_run() as run:\n",
    "    print(f\"Starting MLflow run: {run.info.run_id}\")\n",
    "    \n",
    "    # a. è®°å½•å‚æ•° (è¿™é‡Œå¯ä»¥è®°å½•ç®¡é“çš„å‚æ•°)\n",
    "    mlflow.log_param(\"model_type\", \"LogisticRegression (Spark MLlib)\")\n",
    "    mlflow.log_param(\"feature_cols\", \", \".join(feature_cols))\n",
    "    \n",
    "    # b. è®­ç»ƒæ¨¡å‹\n",
    "    # .fit()ä¼šè§¦å‘æ•´ä¸ªç®¡é“åœ¨Sparké›†ç¾¤(æœ¬åœ°æ¨¡å¼)ä¸Šè¿è¡Œ\n",
    "    print(\"Training distributed model...\")\n",
    "    spark_model = pipeline.fit(features_df)\n",
    "    \n",
    "    # c. è®°å½•æ¨¡å‹ï¼\n",
    "    # mlflow.spark.log_model() ä¼šä¿å­˜æ•´ä¸ªç®¡é“\n",
    "    # è¿™æ ·ï¼Œåœ¨æ¨ç†æ—¶ï¼Œå®ƒä¼šè‡ªåŠ¨å¤„ç†æ–°æ•°æ®çš„StringIndexingå’ŒVectorAssembling\n",
    "    print(\"Logging Spark ML Pipeline to MLflow...\")\n",
    "    mlflow.spark.log_model(\n",
    "        spark_model, \n",
    "        \"spark-pipeline-model\" # äº§ç‰©è·¯å¾„\n",
    "    )\n",
    "    \n",
    "    # (è¯„ä¼°æ¨¡å‹ - ç®€å•èµ·è§ï¼Œæˆ‘ä»¬æš‚ä¸è¿è¡Œè¯„ä¼°)\n",
    "    # predictions = spark_model.transform(test_data)\n",
    "    # ...\n",
    "    mlflow.log_metric(\"training_accuracy\", 0.99) # æš‚æ—¶ç¡¬ç¼–ç ä¸€ä¸ªæŒ‡æ ‡\n",
    "    print(\"Artifact URI:\", mlflow.get_artifact_uri())\n",
    "\n",
    "print(\"\\n--- Spark ML Experiment Run Finished! ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30141d3f-1d65-4603-8fb6-73309a2dc351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fs.s3a.impl = org.apache.hadoop.fs.s3a.S3AFileSystem\n",
      "endpoint = http://minio:9000\n",
      "access.key = minioadmin\n",
      "secret.key = minioadmin\n"
     ]
    }
   ],
   "source": [
    "hc = spark._jsc.hadoopConfiguration()\n",
    "print(\"fs.s3a.impl =\", hc.get(\"fs.s3a.impl\"))\n",
    "print(\"endpoint =\", hc.get(\"fs.s3a.endpoint\"))\n",
    "print(\"access.key =\", hc.get(\"fs.s3a.access.key\"))\n",
    "print(\"secret.key =\", hc.get(\"fs.s3a.secret.key\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9692aaf-2b4f-4d49-9ae9-1797d684a1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.range(1,10).write.mode(\"overwrite\").parquet(\"s3a://mlflow-artifacts/test2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "afaaef67-b21c-460d-bb8f-f28df57b4de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet(\"s3a://mlflow-artifacts/test2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc84e0a3-9e11-4ac9-93ec-5baae91dcc7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  5|\n",
      "|  6|\n",
      "|  7|\n",
      "|  8|\n",
      "|  9|\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28f2eee3-8e01-420a-b55d-c1dcf915b39b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2025-11-13 01:48:25</td></tr>\n",
       "<tr><td>Running for: </td><td>00:00:18.30        </td></tr>\n",
       "<tr><td>Memory:      </td><td>18.8/31.2 GiB      </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using AsyncHyperBand: num_stopped=15<br>Bracket: Iter 64.000: None | Iter 16.000: None | Iter 4.000: None | Iter 1.000: 0.8416666666666667<br>Logical resource usage: 1.0/32 CPUs, 0/0 GPUs\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status    </th><th>loc              </th><th style=\"text-align: right;\">          C</th><th>solver   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  accuracy</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_objective_ccf02_00000</td><td>TERMINATED</td><td>172.18.0.15:75182</td><td style=\"text-align: right;\">0.0231507  </td><td>lbfgs    </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">      0.00438809</td><td style=\"text-align: right;\">  0.966667</td></tr>\n",
       "<tr><td>train_objective_ccf02_00001</td><td>TERMINATED</td><td>172.18.0.15:75129</td><td style=\"text-align: right;\">0.0350876  </td><td>saga     </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">      0.00334787</td><td style=\"text-align: right;\">  0.833333</td></tr>\n",
       "<tr><td>train_objective_ccf02_00002</td><td>TERMINATED</td><td>172.18.0.15:75250</td><td style=\"text-align: right;\">0.00286301 </td><td>liblinear</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">      0.00411177</td><td style=\"text-align: right;\">  0.766667</td></tr>\n",
       "<tr><td>train_objective_ccf02_00003</td><td>TERMINATED</td><td>172.18.0.15:75228</td><td style=\"text-align: right;\">0.00175977 </td><td>saga     </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">      0.00541925</td><td style=\"text-align: right;\">  0.866667</td></tr>\n",
       "<tr><td>train_objective_ccf02_00004</td><td>TERMINATED</td><td>172.18.0.15:75239</td><td style=\"text-align: right;\">0.0247409  </td><td>saga     </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">      0.0034318 </td><td style=\"text-align: right;\">  0.866667</td></tr>\n",
       "<tr><td>train_objective_ccf02_00005</td><td>TERMINATED</td><td>172.18.0.15:75227</td><td style=\"text-align: right;\">0.00436631 </td><td>saga     </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">      0.00362563</td><td style=\"text-align: right;\">  0.7     </td></tr>\n",
       "<tr><td>train_objective_ccf02_00006</td><td>TERMINATED</td><td>172.18.0.15:75165</td><td style=\"text-align: right;\">0.000306437</td><td>lbfgs    </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">      0.0036509 </td><td style=\"text-align: right;\">  0.6     </td></tr>\n",
       "<tr><td>train_objective_ccf02_00007</td><td>TERMINATED</td><td>172.18.0.15:75191</td><td style=\"text-align: right;\">0.00635411 </td><td>liblinear</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">      0.00293064</td><td style=\"text-align: right;\">  0.733333</td></tr>\n",
       "<tr><td>train_objective_ccf02_00008</td><td>TERMINATED</td><td>172.18.0.15:75156</td><td style=\"text-align: right;\">0.00923201 </td><td>saga     </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">      0.00356531</td><td style=\"text-align: right;\">  0.833333</td></tr>\n",
       "<tr><td>train_objective_ccf02_00009</td><td>TERMINATED</td><td>172.18.0.15:75231</td><td style=\"text-align: right;\">0.0232809  </td><td>liblinear</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">      0.0027113 </td><td style=\"text-align: right;\">  0.7     </td></tr>\n",
       "<tr><td>train_objective_ccf02_00010</td><td>TERMINATED</td><td>172.18.0.15:75238</td><td style=\"text-align: right;\">0.00204406 </td><td>lbfgs    </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">      0.00424099</td><td style=\"text-align: right;\">  0.8     </td></tr>\n",
       "<tr><td>train_objective_ccf02_00011</td><td>TERMINATED</td><td>172.18.0.15:75253</td><td style=\"text-align: right;\">0.00222374 </td><td>saga     </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">      0.00448203</td><td style=\"text-align: right;\">  0.566667</td></tr>\n",
       "<tr><td>train_objective_ccf02_00012</td><td>TERMINATED</td><td>172.18.0.15:75226</td><td style=\"text-align: right;\">0.000101488</td><td>lbfgs    </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">      0.00321364</td><td style=\"text-align: right;\">  0.6     </td></tr>\n",
       "<tr><td>train_objective_ccf02_00013</td><td>TERMINATED</td><td>172.18.0.15:75248</td><td style=\"text-align: right;\">0.0256628  </td><td>lbfgs    </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">      0.00448656</td><td style=\"text-align: right;\">  0.9     </td></tr>\n",
       "<tr><td>train_objective_ccf02_00014</td><td>TERMINATED</td><td>172.18.0.15:75251</td><td style=\"text-align: right;\">0.0713145  </td><td>saga     </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">      0.00532174</td><td style=\"text-align: right;\">  1       </td></tr>\n",
       "<tr><td>train_objective_ccf02_00015</td><td>TERMINATED</td><td>172.18.0.15:75143</td><td style=\"text-align: right;\">0.00110374 </td><td>liblinear</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">      0.00302792</td><td style=\"text-align: right;\">  0.333333</td></tr>\n",
       "<tr><td>train_objective_ccf02_00016</td><td>TERMINATED</td><td>172.18.0.15:75249</td><td style=\"text-align: right;\">0.000288839</td><td>lbfgs    </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">      0.00360847</td><td style=\"text-align: right;\">  0.566667</td></tr>\n",
       "<tr><td>train_objective_ccf02_00017</td><td>TERMINATED</td><td>172.18.0.15:75254</td><td style=\"text-align: right;\">0.000380397</td><td>saga     </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">      0.00281477</td><td style=\"text-align: right;\">  0.633333</td></tr>\n",
       "<tr><td>train_objective_ccf02_00018</td><td>TERMINATED</td><td>172.18.0.15:75252</td><td style=\"text-align: right;\">0.0267679  </td><td>saga     </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">      0.00404406</td><td style=\"text-align: right;\">  0.733333</td></tr>\n",
       "<tr><td>train_objective_ccf02_00019</td><td>TERMINATED</td><td>172.18.0.15:75192</td><td style=\"text-align: right;\">0.00305739 </td><td>lbfgs    </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">      0.00432372</td><td style=\"text-align: right;\">  0.666667</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_objective pid=75143)\u001b[0m /opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "\u001b[36m(train_objective pid=75143)\u001b[0m   warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸƒ View run train_objective_ccf02_00007 at: http://mlflow-server:5000/#/experiments/3/runs/1cc133f467eb4ca69327ea7185ebfdea\n",
      "ğŸ§ª View experiment at: http://mlflow-server:5000/#/experiments/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_objective pid=75129)\u001b[0m /opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸƒ View run train_objective_ccf02_00001 at: http://mlflow-server:5000/#/experiments/3/runs/18062b70464743d58f25963a49b5ceec\n",
      "ğŸ§ª View experiment at: http://mlflow-server:5000/#/experiments/3\n",
      "ğŸƒ View run train_objective_ccf02_00016 at: http://mlflow-server:5000/#/experiments/3/runs/6d301f6dfcaf417fb4365cdb830318b5\n",
      "ğŸ§ª View experiment at: http://mlflow-server:5000/#/experiments/3\n",
      "ğŸƒ View run train_objective_ccf02_00010 at: http://mlflow-server:5000/#/experiments/3/runs/472f74171bec429a9afaa8f79dc55622\n",
      "ğŸ§ª View experiment at: http://mlflow-server:5000/#/experiments/3\n",
      "ğŸƒ View run train_objective_ccf02_00015 at: http://mlflow-server:5000/#/experiments/3/runs/01638011306d480d90474a1251b643c1\n",
      "ğŸ§ª View experiment at: http://mlflow-server:5000/#/experiments/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_objective pid=75231)\u001b[0m /opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\u001b[32m [repeated 2x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[36m(train_objective pid=75231)\u001b[0m   warnings.warn(\u001b[32m [repeated 6x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸƒ View run train_objective_ccf02_00008 at: http://mlflow-server:5000/#/experiments/3/runs/e64a0b62d45d4e5d934930b4f3d9ce50\n",
      "ğŸ§ª View experiment at: http://mlflow-server:5000/#/experiments/3\n",
      "ğŸƒ View run train_objective_ccf02_00019 at: http://mlflow-server:5000/#/experiments/3/runs/c7005e3611a645359125762545284300\n",
      "ğŸ§ª View experiment at: http://mlflow-server:5000/#/experiments/3\n",
      "ğŸƒ View run train_objective_ccf02_00004 at: http://mlflow-server:5000/#/experiments/3/runs/5dd35586a43a4e11bd4dd4bc1f4e9399\n",
      "ğŸ§ª View experiment at: http://mlflow-server:5000/#/experiments/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_objective pid=75251)\u001b[0m /opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\u001b[32m [repeated 4x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸƒ View run train_objective_ccf02_00017 at: http://mlflow-server:5000/#/experiments/3/runs/949895c1ddf84de0a1abc49f182a2d64\n",
      "ğŸ§ª View experiment at: http://mlflow-server:5000/#/experiments/3\n",
      "ğŸƒ View run train_objective_ccf02_00013 at: http://mlflow-server:5000/#/experiments/3/runs/9f08db2e40914de1bc19f2da40add4b3\n",
      "ğŸ§ª View experiment at: http://mlflow-server:5000/#/experiments/3\n",
      "ğŸƒ View run train_objective_ccf02_00000 at: http://mlflow-server:5000/#/experiments/3/runs/9b5cc318b7d14deea834b15592fd16a0\n",
      "ğŸ§ª View experiment at: http://mlflow-server:5000/#/experiments/3\n",
      "ğŸƒ View run train_objective_ccf02_00009 at: http://mlflow-server:5000/#/experiments/3/runs/c7f7a9211ad24ac69f50b2a267e49427\n",
      "ğŸ§ª View experiment at: http://mlflow-server:5000/#/experiments/3\n",
      "ğŸƒ View run train_objective_ccf02_00006 at: http://mlflow-server:5000/#/experiments/3/runs/579c190b703c47a7807f7b3d0343789e\n",
      "ğŸ§ª View experiment at: http://mlflow-server:5000/#/experiments/3\n",
      "ğŸƒ View run train_objective_ccf02_00018 at: http://mlflow-server:5000/#/experiments/3/runs/a028d378f4344083a41e943bbe3530c8\n",
      "ğŸ§ª View experiment at: http://mlflow-server:5000/#/experiments/3\n",
      "ğŸƒ View run train_objective_ccf02_00003 at: http://mlflow-server:5000/#/experiments/3/runs/14e44bc2b41d4ec8beb0366a262c77bb\n",
      "ğŸ§ª View experiment at: http://mlflow-server:5000/#/experiments/3\n",
      "ğŸƒ View run train_objective_ccf02_00002 at: http://mlflow-server:5000/#/experiments/3/runs/fc87db282d764e3e9e3db948222555e5\n",
      "ğŸ§ª View experiment at: http://mlflow-server:5000/#/experiments/3\n",
      "ğŸƒ View run train_objective_ccf02_00005 at: http://mlflow-server:5000/#/experiments/3/runs/96b6aa3789f44448a7d50209807efced\n",
      "ğŸ§ª View experiment at: http://mlflow-server:5000/#/experiments/3\n",
      "ğŸƒ View run train_objective_ccf02_00012 at: http://mlflow-server:5000/#/experiments/3/runs/f55df184383f443b8ab6195b76de328c\n",
      "ğŸ§ª View experiment at: http://mlflow-server:5000/#/experiments/3\n",
      "ğŸƒ View run train_objective_ccf02_00014 at: http://mlflow-server:5000/#/experiments/3/runs/7190ed5f2b544612842b42457ef8a5b8\n",
      "ğŸ§ª View experiment at: http://mlflow-server:5000/#/experiments/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-13 01:48:25,181\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/home/jovyan/ray_results/hpo_logistic_regression' in 0.0070s.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸƒ View run train_objective_ccf02_00011 at: http://mlflow-server:5000/#/experiments/3/runs/e80d5ec6deb74de6a51a60cadd8e4299\n",
      "ğŸ§ª View experiment at: http://mlflow-server:5000/#/experiments/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-13 01:48:25,186\tINFO tune.py:1041 -- Total run time: 18.42 seconds (18.29 seconds for the tuning loop).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- è°ƒä¼˜å®Œæˆ! ---\n",
      "æœ€ä½³è¯•éªŒçš„ Accuracy: 1.0000\n",
      "æœ€ä½³è¯•éªŒçš„è¶…å‚æ•° (Config): {'C': 0.07131448120364109, 'solver': 'saga'}\n",
      "\n",
      "è¯·æ£€æŸ¥ä½ çš„ MLflow UI ('http://localhost:5000')\n",
      "åœ¨ 'ray_tune_hpo_experiment' å®éªŒä¸­ï¼Œä½ åº”è¯¥èƒ½çœ‹åˆ°æ‰€æœ‰ 20 æ¬¡è¿è¡Œçš„è®°å½•ã€‚\n",
      "\u001b[33m(raylet)\u001b[0m The node with node id: 3bf8339dee73ab81b0c334692fb13d2704a20a59e87327a5ea5f0040 and address: 172.18.0.15 and node name: 172.18.0.15 has been marked dead because the detector has missed too many heartbeats from it. This can happen when a \t(1) raylet crashes unexpectedly (OOM, etc.) \n",
      "\t(2) raylet has lagging heartbeats due to slow network or busy workload.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m [2025-11-13 02:33:36,139 E 72978 73051] (raylet) agent_manager.cc:86: The raylet exited immediately because one Ray agent failed, agent_name = dashboard_agent.\n",
      "\u001b[33m(raylet)\u001b[0m The raylet fate shares with the agent. This can happen because\n",
      "\u001b[33m(raylet)\u001b[0m - The version of `grpcio` doesn't follow Ray's requirement. Agent can segfault with the incorrect `grpcio` version. Check the grpcio version `pip freeze | grep grpcio`.\n",
      "\u001b[33m(raylet)\u001b[0m - The agent failed to start because of unexpected error or port conflict. Read the log `cat /tmp/ray/session_latest/logs/{dashboard_agent|runtime_env_agent}.log`. You can find the log file structure here https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#logging-directory-structure.\n",
      "\u001b[33m(raylet)\u001b[0m - The agent is killed by the OS (e.g., out of memory).\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from ray import tune, train\n",
    "from ray.air.integrations.mlflow import MLflowLoggerCallback\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "\n",
    "import mlflow\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"ray\")\n",
    "\n",
    "# --- 1. è¿æ¥åˆ°æˆ‘ä»¬çš„å¤–éƒ¨æœåŠ¡ ---\n",
    "\n",
    "# a. è¿æ¥åˆ° Ray Head èŠ‚ç‚¹\n",
    "# (ä½¿ç”¨æˆ‘ä»¬åœ¨docker-composeä¸­è®¾ç½®çš„å®¢æˆ·ç«¯ç«¯å£10001)\n",
    "try:\n",
    "    # Ray 2.x çš„æ ‡å‡†è¿æ¥æ–¹å¼\n",
    "    # address=\"ray://<service_name>:<port>\"\n",
    "    ray.init(address=\"ray://ray-head:10001\")\n",
    "    print(\"--- Ray Client: æˆåŠŸè¿æ¥åˆ° Ray Head ---\")\n",
    "except Exception as e:\n",
    "    print(f\"--- Ray Client: è¿æ¥å¤±è´¥: {e} ---\")\n",
    "    print(\"è¯·ç¡®ä¿ ray-head å®¹å™¨æ­£åœ¨è¿è¡Œï¼Œå¹¶ä¸”ç«¯å£ 10001 å·²æš´éœ²ã€‚\")\n",
    "\n",
    "# b. é…ç½® MLflow å®¢æˆ·ç«¯\n",
    "os.environ[\"MLFLOW_TRACKING_URI\"] = \"http://mlflow-server:5000\"\n",
    "os.environ[\"MLFLOW_S3_ENDPOINT_URL\"] = \"http://minio:9000\"\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = \"minioadmin\"\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"minioadmin\"\n",
    "\n",
    "print(f\"MLFLOW_TRACKING_URI set to: {os.environ['MLFLOW_TRACKING_URI']}\")\n",
    "\n",
    "# --- 2. å®šä¹‰ç›®æ ‡å‡½æ•° (Ray Tuneå°†è¿è¡Œè¿™ä¸ªå‡½æ•°) ---\n",
    "\n",
    "def train_objective(config: dict):\n",
    "    \"\"\"\n",
    "    Ray Tune å°†åœ¨å®ƒè‡ªå·±çš„è¿›ç¨‹ä¸­(æˆ–Workerä¸Š)è¿è¡Œè¿™ä¸ªå‡½æ•°ã€‚\n",
    "    'config' å­—å…¸åŒ…å«äº†Rayä»æœç´¢ç©ºé—´ä¸­ä¸ºè¿™æ¬¡\"Trial\"é€‰æ‹©çš„å‚æ•°ã€‚\n",
    "    \"\"\"\n",
    "    \n",
    "    # a. å‡†å¤‡æ•°æ® (æ¯æ¬¡è¿è¡Œæ—¶éƒ½ç‹¬ç«‹åŠ è½½)\n",
    "    X, y = load_iris(return_X_y=True)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "    \n",
    "    # b. ä»Ray Tuneè·å–è¶…å‚æ•°\n",
    "    params = {\n",
    "        \"C\": config[\"C\"],\n",
    "        \"solver\": config[\"solver\"]\n",
    "    }\n",
    "    \n",
    "    # c. è®­ç»ƒæ¨¡å‹\n",
    "    model = LogisticRegression(**params)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # d. è¯„ä¼°æ¨¡å‹\n",
    "    accuracy = accuracy_score(y_test, model.predict(X_test))\n",
    "    \n",
    "    # e. (å…³é”®!) å°†ç»“æœæŠ¥å‘Šå› Ray Tune\n",
    "    tune.report({\"accuracy\": accuracy})\n",
    "\n",
    "# --- 3. å®šä¹‰è¶…å‚æ•°æœç´¢ç©ºé—´ ---\n",
    "\n",
    "search_space = {\n",
    "    \"C\": tune.loguniform(1e-4, 1e-1),\n",
    "    \"solver\": tune.choice([\"liblinear\", \"saga\", \"lbfgs\"])\n",
    "}\n",
    "\n",
    "# --- 4. é…ç½® MLflow é›†æˆ ---\n",
    "mlflow_callback = MLflowLoggerCallback(\n",
    "    tracking_uri=\"http://mlflow-server:5000\",\n",
    "    experiment_name=\"ray_tune_hpo_experiment\",\n",
    "    save_artifact=False # æˆ‘ä»¬åªè®°å½•å‚æ•°å’ŒæŒ‡æ ‡\n",
    ")\n",
    "\n",
    "run_config = tune.RunConfig(\n",
    "    name=\"hpo_logistic_regression\",\n",
    "    callbacks=[mlflow_callback]\n",
    ")\n",
    "\n",
    "# --- 5. é…ç½® Tuner (è°ƒä¼˜å™¨) ---\n",
    "tune_config = tune.TuneConfig(\n",
    "    metric=\"accuracy\",     # æˆ‘ä»¬çš„ä¼˜åŒ–ç›®æ ‡\n",
    "    mode=\"max\",            # æˆ‘ä»¬å¸Œæœ›æœ€å¤§åŒ–è¿™ä¸ªæŒ‡æ ‡\n",
    "    num_samples=20,        # æ€»å…±å°è¯• 20 ç§ä¸åŒçš„å‚æ•°ç»„åˆ\n",
    "    scheduler=ASHAScheduler()  # \n",
    ")\n",
    "\n",
    "# --- 6. å¯åŠ¨è°ƒä¼˜ä½œä¸šï¼ ---\n",
    "print(\"--- å¯åŠ¨ Ray Tune è¶…å‚æ•°è°ƒä¼˜ (å…± 20 æ¬¡è¯•éªŒ) ---\")\n",
    "\n",
    "tuner = tune.Tuner(\n",
    "    train_objective,\n",
    "    param_space=search_space,\n",
    "    tune_config=tune_config,\n",
    "    run_config=run_config\n",
    ")\n",
    "\n",
    "results = tuner.fit()\n",
    "\n",
    "print(\"--- è°ƒä¼˜å®Œæˆ! ---\")\n",
    "\n",
    "# 7. æ‰“å°æœ€ä½³ç»“æœ\n",
    "best_result = results.get_best_result(metric=\"accuracy\", mode=\"max\")\n",
    "print(f\"æœ€ä½³è¯•éªŒçš„ Accuracy: {best_result.metrics['accuracy']:.4f}\")\n",
    "print(f\"æœ€ä½³è¯•éªŒçš„è¶…å‚æ•° (Config): {best_result.config}\")\n",
    "\n",
    "print(\"\\nè¯·æ£€æŸ¥ä½ çš„ MLflow UI ('http://localhost:5000')\")\n",
    "print(\"åœ¨ 'ray_tune_hpo_experiment' å®éªŒä¸­ï¼Œä½ åº”è¯¥èƒ½çœ‹åˆ°æ‰€æœ‰ 20 æ¬¡è¿è¡Œçš„è®°å½•ã€‚\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8743bb99-5e30-4383-83bc-39067a782a70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status Code: 200\n",
      "Prediction Response:\n",
      "{'predictions': [0, 2]}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# (å‡è®¾ä½ éƒ¨ç½²çš„æ˜¯sklearnæ¨¡å‹ï¼Œè¾“å…¥æ˜¯Pandas DataFrameæ ¼å¼)\n",
    "test_data = {\n",
    "    \"dataframe_split\": {\n",
    "        \"columns\": [\"sepal length (cm)\", \"sepal width (cm)\", \"petal length (cm)\", \"petal width (cm)\"],\n",
    "        \"data\": [\n",
    "            [5.1, 3.5, 1.4, 0.2],\n",
    "            [6.2, 3.4, 5.4, 2.3]\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "# APIç«¯ç‚¹çš„åœ°å€\n",
    "url = \"http://mlflow-model-server:5001/invocations\"\n",
    "\n",
    "headers = {'Content-Type': 'application/json'}\n",
    "\n",
    "# å‘é€POSTè¯·æ±‚\n",
    "response = requests.post(url, data=json.dumps(test_data), headers=headers)\n",
    "\n",
    "print(f\"Status Code: {response.status_code}\")\n",
    "print(\"Prediction Response:\")\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb3256d-218e-4d60-8f3e-0fe269aaa9fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
